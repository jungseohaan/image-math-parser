# llm_tracker.py
"""
LLM API ì‚¬ìš©ëŸ‰ ì¶”ì  ëª¨ë“ˆ
- ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰ ì¶”ì 
- í† í° ì¹´ìš´íŒ… (ì…ë ¥/ì¶œë ¥)
- ë¹„ìš© ì¶”ì •
- íŒŒì¼ ì €ì¥/ë¡œë“œë¡œ ëˆ„ì  í†µê³„ ìœ ì§€
"""

import time
import os
from datetime import datetime
from threading import Lock
from dataclasses import dataclass, field, asdict
from typing import Optional
import json

# í†µê³„ ì €ì¥ íŒŒì¼ ê²½ë¡œ (GEN_DATA_PATH í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
from dotenv import load_dotenv
load_dotenv()

GEN_DATA_PATH = os.path.expanduser(os.environ.get('GEN_DATA_PATH', '~/.gen-data'))
STATS_FILE = os.path.join(GEN_DATA_PATH, 'data', 'llm_stats.json')


# Gemini ëª¨ë¸ë³„ ê°€ê²© (1000 í† í°ë‹¹ USD, 2025ë…„ ê¸°ì¤€)
GEMINI_PRICING = {
    "gemini-2.5-pro": {
        "input": 0.00125,      # $1.25 per 1M tokens
        "output": 0.01         # $10.00 per 1M tokens
    },
    "gemini-2.5-flash": {
        "input": 0.000015,     # $0.015 per 1M tokens
        "output": 0.00006      # $0.06 per 1M tokens
    },
    "gemini-2.0-flash": {
        "input": 0.00001875,   # $0.01875 per 1M tokens = $0.00001875 per 1K
        "output": 0.000075     # $0.075 per 1M tokens = $0.000075 per 1K
    },
    "gemini-2.0-flash-exp": {
        "input": 0.00001875,
        "output": 0.000075
    },
    "gemini-1.5-flash": {
        "input": 0.00001875,
        "output": 0.000075
    },
    "gemini-1.5-pro": {
        "input": 0.00125,      # $1.25 per 1M tokens
        "output": 0.005        # $5.00 per 1M tokens
    }
}

# ê¸°ë³¸ ê°€ê²© (ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ìš©)
DEFAULT_PRICING = {
    "input": 0.0001,
    "output": 0.0003
}


@dataclass
class APICall:
    """ë‹¨ì¼ API í˜¸ì¶œ ì •ë³´"""
    timestamp: str
    model: str
    operation: str  # 'analyze', 'generate_variants', 'verify', 'fix_error' ë“±
    input_tokens: int
    output_tokens: int
    input_cost: float
    output_cost: float
    total_cost: float
    latency_ms: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class UsageStats:
    """ì„¸ì…˜ ì‚¬ìš©ëŸ‰ í†µê³„"""
    total_calls: int = 0
    successful_calls: int = 0
    failed_calls: int = 0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost: float = 0.0
    calls_by_model: dict = field(default_factory=dict)
    calls_by_operation: dict = field(default_factory=dict)
    call_history: list = field(default_factory=list)
    session_start: str = field(default_factory=lambda: datetime.now().isoformat())


class LLMTracker:
    """LLM API ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°"""

    _instance = None
    _lock = Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self._initialized = True
        self._call_lock = Lock()
        self.stats = self._load_stats()

    def _load_stats(self) -> UsageStats:
        """íŒŒì¼ì—ì„œ í†µê³„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
        try:
            if os.path.exists(STATS_FILE):
                with open(STATS_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    stats = UsageStats(
                        total_calls=data.get('total_calls', 0),
                        successful_calls=data.get('successful_calls', 0),
                        failed_calls=data.get('failed_calls', 0),
                        total_input_tokens=data.get('total_input_tokens', 0),
                        total_output_tokens=data.get('total_output_tokens', 0),
                        total_cost=data.get('total_cost', 0.0),
                        calls_by_model=data.get('calls_by_model', {}),
                        calls_by_operation=data.get('calls_by_operation', {}),
                        call_history=data.get('call_history', [])[-100:],  # ìµœê·¼ 100ê°œë§Œ
                        session_start=data.get('first_call', datetime.now().isoformat())
                    )
                    print(f"ğŸ“Š LLM í†µê³„ ë¡œë“œ ì™„ë£Œ: ì´ {stats.total_calls}íšŒ í˜¸ì¶œ, ${stats.total_cost:.6f}")
                    return stats
        except Exception as e:
            print(f"âš ï¸ LLM í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return UsageStats()

    def _save_stats(self):
        """í†µê³„ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(STATS_FILE), exist_ok=True)

            data = {
                'total_calls': self.stats.total_calls,
                'successful_calls': self.stats.successful_calls,
                'failed_calls': self.stats.failed_calls,
                'total_input_tokens': self.stats.total_input_tokens,
                'total_output_tokens': self.stats.total_output_tokens,
                'total_cost': self.stats.total_cost,
                'calls_by_model': self.stats.calls_by_model,
                'calls_by_operation': self.stats.calls_by_operation,
                'call_history': self.stats.call_history[-100:],  # ìµœê·¼ 100ê°œë§Œ
                'first_call': self.stats.session_start,
                'last_updated': datetime.now().isoformat()
            }

            with open(STATS_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ LLM í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")

    def estimate_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤ (ëŒ€ëµì ì¸ ê³„ì‚°)"""
        if not text:
            return 0
        # í•œê¸€ì€ ì•½ 1.5ê¸€ìë‹¹ 1í† í°, ì˜ì–´ëŠ” ì•½ 4ê¸€ìë‹¹ 1í† í°
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7a3')
        other_chars = len(text) - korean_chars
        estimated = int(korean_chars / 1.5 + other_chars / 4)
        return max(estimated, 1)

    def get_pricing(self, model: str) -> dict:
        """ëª¨ë¸ì˜ ê°€ê²© ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
        return GEMINI_PRICING.get(model, DEFAULT_PRICING)

    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> tuple:
        """ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
        pricing = self.get_pricing(model)
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost, output_cost, input_cost + output_cost

    def track_call(
        self,
        model: str,
        operation: str,
        prompt: str,
        response_text: str,
        latency_ms: float,
        success: bool = True,
        error_message: str = None,
        response_metadata: dict = None
    ) -> APICall:
        """API í˜¸ì¶œì„ ì¶”ì í•©ë‹ˆë‹¤"""

        # í† í° ìˆ˜ ê³„ì‚° (Gemini ì‘ë‹µ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©)
        if response_metadata and 'usage_metadata' in response_metadata:
            usage = response_metadata['usage_metadata']
            input_tokens = usage.get('prompt_token_count', self.estimate_tokens(prompt))
            output_tokens = usage.get('candidates_token_count', self.estimate_tokens(response_text))
        else:
            input_tokens = self.estimate_tokens(prompt)
            output_tokens = self.estimate_tokens(response_text) if response_text else 0

        # ë¹„ìš© ê³„ì‚°
        input_cost, output_cost, total_cost = self.calculate_cost(model, input_tokens, output_tokens)

        # í˜¸ì¶œ ì •ë³´ ìƒì„±
        call = APICall(
            timestamp=datetime.now().isoformat(),
            model=model,
            operation=operation,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=total_cost,
            latency_ms=latency_ms,
            success=success,
            error_message=error_message
        )

        # í†µê³„ ì—…ë°ì´íŠ¸
        with self._call_lock:
            self.stats.total_calls += 1
            if success:
                self.stats.successful_calls += 1
            else:
                self.stats.failed_calls += 1

            self.stats.total_input_tokens += input_tokens
            self.stats.total_output_tokens += output_tokens
            self.stats.total_cost += total_cost

            # ëª¨ë¸ë³„ í†µê³„
            if model not in self.stats.calls_by_model:
                self.stats.calls_by_model[model] = {
                    "calls": 0, "input_tokens": 0, "output_tokens": 0, "cost": 0.0
                }
            self.stats.calls_by_model[model]["calls"] += 1
            self.stats.calls_by_model[model]["input_tokens"] += input_tokens
            self.stats.calls_by_model[model]["output_tokens"] += output_tokens
            self.stats.calls_by_model[model]["cost"] += total_cost

            # ì‘ì—…ë³„ í†µê³„
            if operation not in self.stats.calls_by_operation:
                self.stats.calls_by_operation[operation] = {
                    "calls": 0, "input_tokens": 0, "output_tokens": 0, "cost": 0.0
                }
            self.stats.calls_by_operation[operation]["calls"] += 1
            self.stats.calls_by_operation[operation]["input_tokens"] += input_tokens
            self.stats.calls_by_operation[operation]["output_tokens"] += output_tokens
            self.stats.calls_by_operation[operation]["cost"] += total_cost

            # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            self.stats.call_history.append({
                "timestamp": call.timestamp,
                "model": call.model,
                "operation": call.operation,
                "input_tokens": call.input_tokens,
                "output_tokens": call.output_tokens,
                "total_cost": call.total_cost,
                "latency_ms": call.latency_ms,
                "success": call.success
            })
            if len(self.stats.call_history) > 100:
                self.stats.call_history = self.stats.call_history[-100:]

            # íŒŒì¼ì— ì €ì¥
            self._save_stats()

        return call

    def get_stats(self) -> dict:
        """í˜„ì¬ ì‚¬ìš©ëŸ‰ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
        with self._call_lock:
            return {
                "session_start": self.stats.session_start,
                "total_calls": self.stats.total_calls,
                "successful_calls": self.stats.successful_calls,
                "failed_calls": self.stats.failed_calls,
                "total_input_tokens": self.stats.total_input_tokens,
                "total_output_tokens": self.stats.total_output_tokens,
                "total_tokens": self.stats.total_input_tokens + self.stats.total_output_tokens,
                "total_cost_usd": round(self.stats.total_cost, 6),
                "total_cost_krw": round(self.stats.total_cost * 1350, 2),  # ëŒ€ëµì ì¸ í™˜ìœ¨
                "by_model": dict(self.stats.calls_by_model),
                "by_operation": dict(self.stats.calls_by_operation),
                "recent_calls": self.stats.call_history[-10:]  # ìµœê·¼ 10ê°œ í˜¸ì¶œ
            }

    def reset_stats(self):
        """í†µê³„ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"""
        with self._call_lock:
            self.stats = UsageStats()
            # íŒŒì¼ë„ ì´ˆê¸°í™”
            self._save_stats()
            print("ğŸ“Š LLM í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_summary(self) -> str:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤"""
        stats = self.get_stats()
        lines = [
            f"ğŸ“Š LLM ì‚¬ìš©ëŸ‰ í†µê³„",
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            f"ì´ í˜¸ì¶œ ìˆ˜: {stats['total_calls']} (ì„±ê³µ: {stats['successful_calls']}, ì‹¤íŒ¨: {stats['failed_calls']})",
            f"ì´ í† í°: {stats['total_tokens']:,} (ì…ë ¥: {stats['total_input_tokens']:,}, ì¶œë ¥: {stats['total_output_tokens']:,})",
            f"ì´ ë¹„ìš©: ${stats['total_cost_usd']:.6f} (ì•½ â‚©{stats['total_cost_krw']:.2f})",
            "",
            "ğŸ“ˆ ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰:"
        ]

        for model, data in stats['by_model'].items():
            lines.append(f"  â€¢ {model}: {data['calls']}íšŒ, {data['input_tokens'] + data['output_tokens']:,} í† í°, ${data['cost']:.6f}")

        lines.append("")
        lines.append("ğŸ”§ ì‘ì—…ë³„ ì‚¬ìš©ëŸ‰:")
        for op, data in stats['by_operation'].items():
            lines.append(f"  â€¢ {op}: {data['calls']}íšŒ, {data['input_tokens'] + data['output_tokens']:,} í† í°")

        return "\n".join(lines)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
tracker = LLMTracker()


def track_gemini_call(operation: str):
    """Gemini API í˜¸ì¶œì„ ì¶”ì í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                latency_ms = (time.time() - start_time) * 1000

                # ì‘ë‹µì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„
                response_metadata = None
                response_text = ""

                if hasattr(result, 'text'):
                    response_text = result.text
                if hasattr(result, '_result'):
                    response_metadata = {'usage_metadata': getattr(result._result, 'usage_metadata', None)}

                # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ (argsë‚˜ kwargsì—ì„œ)
                prompt = ""
                if args:
                    for arg in args:
                        if isinstance(arg, str):
                            prompt = arg
                            break
                        elif isinstance(arg, list):
                            for item in arg:
                                if isinstance(item, str):
                                    prompt += item + "\n"

                tracker.track_call(
                    model="gemini-2.0-flash",  # ê¸°ë³¸ê°’
                    operation=operation,
                    prompt=prompt,
                    response_text=response_text,
                    latency_ms=latency_ms,
                    success=True,
                    response_metadata=response_metadata
                )

                return result
            except Exception as e:
                latency_ms = (time.time() - start_time) * 1000
                tracker.track_call(
                    model="gemini-2.0-flash",
                    operation=operation,
                    prompt="",
                    response_text="",
                    latency_ms=latency_ms,
                    success=False,
                    error_message=str(e)
                )
                raise
        return wrapper
    return decorator
